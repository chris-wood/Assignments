\documentclass[11pt]{article}

\usepackage{thumbpdf, amssymb, amsmath, amsthm, microtype,
	    graphicx, verbatim, listings, color, fancybox}
\usepackage[pdftex]{hyperref}
%\usepackage[margin=1in]{geometry}
\usepackage{cawsty}
\usepackage{fullpage}
\usepackage{pseudocode}
\usepackage{verbatim}

\newcommand{\tlg}{\text{ lg}}
\newcommand{\tln}{\text{ ln}}
\newcommand{\tlog}{\text{ log}}

%\setlength{\parindent}{0pt}

\linespread{1.2}

\begin{document}
\arltitle{4005-800 Algorithms}{Homework 2}
\begin{prob}{1}
\end{prob}
\begin{sol} 

The time complexity of the recurrence $fib$ can be characterized using a recurrence relation that defines the number of recursive calls made by $F_{n}$, as shown below.
\begin{eqnarray*}
T_{F}(0) & = & 1 \\
T_{F}(1) & = & 1 \\
T_{F}(n) & = & T_{F}(n-1) + T_{F}(n-2)
\end{eqnarray*}

The solution to $T_{F}(n)$ can be solved by making the observation that $T_{F}(n) = F_{n+1}$, which is proved below using strong induction.\\

\textbf{Base ($n = 0$)} \\
By definition we know that $T_{F}(0) = 1$ and $F_{0 + 1} = F_{1} = 1$. Thus, $T_{F}(0) = F_{1}$.

\textbf{Base ($n = 1$)} \\
By definition we know that $T_{F}(1) = 1$ and $F_{1 + 1} = F_{2} = F_{1} + F_{0} = 0 + 1 = 1$. Thus, $T_{F}(1) = F_{2}$.

\textbf{Induction ($n > 1$)} \\
Assume that $T_{F}(k) = F_{k + 1}$ for all $k$ such that $2 \leq k < n$. We will show that $T_{F}(n) = F_{n + 1}$.
\begin{eqnarray*}
T_{F}(n) & = & T_{F}(n - 1) + T_{F}(n - 2) \text{ (by definition of $T_{F}$)} \\
& = & F_{n} + F_{n - 1} \text{ (by the induction hypothesis)} \\
& = & F_{n + 1} \text{ (by definition of $F_{n}$) }
\end{eqnarray*}

The solution to the recurrence $T_{F}(n) = F_{n+1}$ can then be solved by treating it as a homogeneous second-order linear recurrence with constant coefficients, which yields the result that $T_{F}(n) = F_{n+1} = \frac{1}{\sqrt{5}}(\phi^{n+1} - \phi'^{n+1})$, where $\phi = \frac{1 + \sqrt{5}}{2}$ and $\phi' = \frac{1 - \sqrt{5}}{2}$. Thus, we can express this solution, and subsequently the time complexity of $fib$, as a function of exponential growth of $n$, as shown below. 
\begin{eqnarray*}
T_{F}(n) \in \Theta(\phi^{n}) \implies T_{F}(n) = \Theta(\phi^{n})
\end{eqnarray*}

However, without loss of generality, we can also solve $T_{F}(n)$ using the substitution approach, knowing that $T_{F}(n) = \Theta(a^{n})$ for some $a$, where $a$ is a real number.
\begin{eqnarray*}
T_{F}(n) = T_{F}(n-1) + T_{F}(n-2) \\
a^{n} = a^{n-1} + a^{n-2} \text{ (by substitution)}\\
a^{2}  =  a + 1 \text{ (divide by $a^{n-2}$)} \\
a^{2} - a - 1 = 0 \text{ (move terms to one side)}
\end{eqnarray*}

At this point one can see we have a very simply quadratic equation with two roots. We can solve for these roots using the quadratic equation, which yields $a = \frac{1 \pm \sqrt{5}}{2}$. Thus, since $a = \frac{1 + \sqrt{5}}{2} = \phi$ is the larger of the two roots and $a = \frac{1 - \sqrt{5}}{2}$ is negative, we can conclude that $a = \frac{1 + \sqrt{5}}{2} = \phi$, which implies that $T_{F}(n) = \Theta(\phi^{n})$.

%0 &=& a^{n} - a^{n-1} - a^{n-2}  \text{ (move terms to one side)}\\
%0 &=& a^{2} - a - 1  \text{ (divide by $a^{n-2}$)}\\
%a &=& \frac{1 \pm \sqrt(5)}{2} \text{ (by quadratic equation for polynomial roots)}
%\end{eqnarray*}

\end{sol}

\begin{prob}{2}
\end{prob}
\begin{sol}

The time complexity of the $fibIt$ routine can be found by solving the recurrence relation that defines $fibIt$. Such a recurrence relation can be defined by analyzing the number of additions performed during each call to $fibIt$, which is captured in the following set of equations.
\begin{eqnarray*}
T_{f}(0) & = & 0 \\
T_{f}(1) & = & 0 \\
T_{f}(n) & = & T_{f}(n-1) + 1
\end{eqnarray*}

This is because there is only one addition made in each recursive call from $f(n;a,b)$ to $f(n-1;b,a+b)$, and there are no additions made in the two cases where $n = 0$ and $n = 1$.

In order to solve this recurrence relation we can expand out the expression and attempt to identify the pattern (i.e. the \textbf{method of iteration}). This process is shown below.
\begin{eqnarray*}
T_{f}(n) & = & T_{f}(n - 1) + 1 \\
& = & (T_{f}(n - 2) + 1) + 1 = T_{f}(n - 2) + 2 \\
& = & (T_{f}(n - 3) + 1) + 2 = T_{f}(n - 3) + 3 \\
& = & ... \\
& = & (T_{f}(n - k) + 1) + k = T_{f}(n - k) + k
\end{eqnarray*}

Based on this pattern, we can reach the first base case of this recurrence relation ($T_{f}(1)$) when $(n - k) = 1$, meaning that $k = (n - 1)$. Thus, we have the following.
\begin{eqnarray*}
T_{f}(n) & = & T_{f}(n - (n - 1)) + (n - 1)  \\
& = & T_{f}(1) + (n - 1) \\
& = & 0 + (n - 1) \\
& = & n - 1
\end{eqnarray*}

Based on this observation we can clearly see that $T_{f}(n) \in \Theta(n - 1) = \Theta(n)$, or simply $T_{f}(n) = \Theta(n)$. Therefore, the time complexity of $fibIt$ is $\Theta(n)$.

%However, to verify this, we perform the substitution method on this recurrence relation. To start, we also %assume that $T_{f}(n - 1) \in \Theta(n)$. Then, we perform the induction as follows.

%\begin{eqnarray*}
%T_{f}(n) & = & T_{f}(n - 1) + 1 \\
%& = & \Theta(n) + 1 \\
%& = & n + (n - 1) \\
%& = & n - 1
%\end{eqnarray*}

%%TODO: is it necessary to perform substitution to solve the rest of this?

\end{sol}

\begin{prob}{3}
\end{prob}
\begin{sol}

\textbf{Base ($n = 0$)} \\
When $n = 0$, we know that $L^{0}(a,b) = (a,b)$ because the operator $L$ is applied $0$ times to $(a,b)$. Furthermore, by definition of $f$, we know that $(f(0;a,b), f(1;a,b)) = (a,b)$. Thus, $L^{0}(a,b) = (f(0;a,b), f(1;a,b))$. \\
%\begin{eqnarray*}
%L^{0}(a,b) & = & (f(0;a,b),f(1;a,b)) \text{ (by the induction hypothesis)}\\
%& = & (a, b) \text{ (by definition of $f$)}
%\end{eqnarray*}

\textbf{Induction ($n > 0$)} \\
First, we assume that $L^{n}(a,b) = (f(n;a,b),f(n + 1;a,b))$. Now we show that $L^{n+1}(a,b) = (f(n + 1;a,b),f(n + 2;a,b))$.
\begin{eqnarray*}
L^{n + 1}(a,b) & = & L(L^{n}(a,b)) \text{ (by law of exponents)}\\
& = & L(f(n;a,b),f(n + 1;a,b)) \text{ (by the induction hypothesis)} \\
& = & (f(n + 1;a,b), f(n;a,b) + f(n + 1;a,b)) \text{ (by definition of $L$)} \\
& = & (f(n + 1;a,b), f(n + 2;a,b)) \text{ (by Theorem 1)}
\end{eqnarray*}

Thus, $L^{n+1}(a,b) = (f(n + 1;a,b),f(n + 2;a,b))$, as desired. Therefore, we know that $f(n;a,b) = (L^{n}(a,b))_{1}$.

\end{sol}

\begin{prob}{4-a}
\end{prob}
\begin{sol}

L can be represented as the product of two matrices, as shown below.
\begin{center}
$L\begin{pmatrix} a \\ b \end{pmatrix} = \begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} a \\ b \end{pmatrix} = \begin{pmatrix} b \\ a+b \end{pmatrix}$
\end{center}

\begin{center}
$L^{n}\begin{pmatrix} a \\ b \end{pmatrix} = \begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix}^{n} \begin{pmatrix} a \\ b \end{pmatrix} = \begin{pmatrix} f(n;a,b) \\ f(n+1;a,b) \end{pmatrix}$
\end{center}
\end{sol} 

\begin{prob}{4-b}
\end{prob}
\begin{sol}

We can use the method of repeated squaring to achieve fast exponentiation of these matrices in $\Theta(\tlog n)$ time. The recursive definition for this algorithm is shown below.

\[
a^{n} = \left\{ 
  \begin{array}{l l}
    (a^{\frac{n}{2}})^{2}, & \quad \text{ if n is even and positive,} \\
    (a^{\frac{(n-1)}{2}})^{2} * a, & \quad \text{ if n is odd}, \\
   1 & \quad \text{ if n = 0}
  \end{array} \right.
\]

Careful analysis reveals that this algorithm runs in $\Theta(\tlog n)$ time because, on each iteration, the size is reduced by about a half at the expense of one or two multiplications \cite{AlgoBook}. This assumes of course that the time for multiplication is $\Theta(1)$.

The source code for this routine (which can be implemented both recursively and iteratively) is shown below.
\begin{lstlisting}
def matrixPower(base, p):
	if (p == 0):
		return IDENTITY
	elif (p == 1):
		return base
	elif ((p % 2) == 0):
		return matrixPower(base * base, p / 2)
	else:
		return base * matrixPower(base * base, (p - 1) / 2)
\end{lstlisting}

\begin{lstlisting}
def matrixPower(base, p):
	result = IDENTITY
	while (p != 0):
		if ((p % 2) != 0):
			result = result * base
			p = p - 1
		base = base * base
		p = p / 2
	return result
\end{lstlisting}
Finally, we can implement a single $power$ routine that uses either one of these multiplication functions to raise objects of type $L$ to the $n$th power that is $\Theta(tlog n)$, as shown below.
\begin{lstlisting}
def power(base, p):
	matrix = matrixPower(base.m, p)
	v1 = (matrix[0] * base.a) + (matrix[1] * base.b)
	v2 = (matrix[2] * base.a) + (matrix[3] * base.b)
	return (v1, v2)
\end{lstlisting}
\end{sol}

\begin{prob}{4-c}
\end{prob}
\begin{sol}

Using the representation for $L$ and the power functions described above, we can implement $fibPow$ as follows:
\begin{lstlisting}
def fibPow(n, a, b):
	base = L(a,b,0,1,1,1)
	return base.power(n)
\end{lstlisting}
\end{sol}

\begin{prob}{4-d}
\end{prob}
\begin{sol}

Since the time to perform matrix multiplication with a $2x2$ and $2x1$ matrix is constant time (i.e. $\Theta(1)$), and the multiplication routine of repeated squares that utilizes this constant operation runs in $\Theta(\tlog n)$ time, we can conclude that the time complexity of $fibPow$ is $\Theta(\tlog n)$.
\end{sol}

\begin{prob}{5-a}
Write down the definition of pseudo-polynomial time.
\end{prob}
\begin{sol}
\begin{define}
A pseudo-polynomial time algorithm is one that runs in polynomial time (i.e. $\Theta(n^{k}), n \geq 1$) with respect to the value $n$ of its input, but does not run in polynomial time with respect to the size $b$ of its input (i.e. it must run in exponential time with respect to the size of the input, or simply $\Theta(m^{b}), m > 1$). In other words, when working with numerical input $n$ to an algorithm, we can say that such an algorithm runs in pseudo-polynomial time if its time complexity is polynomial with respect to $n$ and exponential with respect to $b = \tlg (n)$, because $b$ represents the approximate number of bits used to represent $n$ (i.e. the size of $n$).
%\cite{BELOW}
% http://www.esi2.us.es/~mbilbao/complexi.htm
\end{define}
\end{sol}

\begin{prob}{5-b}
Is $fib$ a pseudo-polynomial time algorithm? Explain.
\end{prob}
\begin{sol}

No, $fib$ has a time complexity of $\Theta(\phi^{n})$, where $n$ is the input value, which means that it is exponential with respect to the value of the input, which further implies that it is not polynomial with respect to the value of the input. Therefore, by definition, $fib$ is not a pseudo-polynomial time algorithm.
\end{sol}

\begin{prob}{5-c}
Is $fibIt$ a pseudo-polynomial time algorithm? Explain.
\end{prob}
\begin{sol}

Yes, $fibIt$ has a time complexity of $\Theta(n)$, which can also be defined as $\Theta(2^{\tlg n})$, where $n$ is the input value and $\tlg (n)$ is the bit size of $n$. This means that it $fibIt$ has polynomial time complexity with respect to the value of $n$ and exponential time with respect to the size of $n$. Therefore, by definition, $fibIt$ is a pseudo-polynomial time algorithm.
\end{sol}

\begin{prob}{5-d}
Is $fibPow$ a pseudo-polynomial time algorithm? Explain.
\end{prob}
\begin{sol}

No, $fibPow$ has a time complexity of $\Theta(\tlg n)$, which can also be defined as $\Theta(2^{\tlg \tlg n})$, where $n$ is the input value and $\tlg n$ is the bit size of $n$. This means that $fibPow$ has a sub-linear time complexity with respect to the value of $n$ (i.e. it does not have a time complexity of $\Theta(n^{k}), k \geq 1$) and thus sub-exponential time complexity with respect to the size of $n$. Therefore, since $fibPow$ does not have polynomial runtime in the value of $n$, nor does it have exponential time complexity in the size of $n$, it is not a pseudo-polynomial time algorithm.

%No, $fibPow$ has a time complexity of $\Theta(\tlg n)$, which can also be defined as $\Theta(2^{\tlg \tlg n})$, where $n$ is the input value, which means that it is sub-polynomial in the magnitude of $n$ and sub-exponential in the bit size of $n$. Therefore, by definition, $fibPow$ is not a pseudo-polynomial time algorithm.

%You can write n^(log(n)) as (k^(logk(n)))^(log(n)) = k^(K*(log(n)^2)). Since (log(n))^2 < n for n large enough, then this means that n^(log(n)) will grow slower than k^n
\end{sol}

\begin{thebibliography}{}
\bibitem{AlgoBook}
Levitin, Anany. {\it Introduction to the Design and Analysis of Algorithms.} Pearson, Boston: 2012. Print.
\end{thebibliography}


\end{document}
