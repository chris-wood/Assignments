\documentclass[11pt]{article}

\usepackage{thumbpdf, amssymb, amsmath, amsthm, microtype,
	    graphicx, verbatim, listings, color, fancybox}
\usepackage[pdftex]{hyperref}
%\usepackage[margin=1in]{geometry}
\usepackage{cawsty}
\usepackage{fullpage}
\usepackage{pseudocode}
\usepackage{verbatim}

\newcommand{\tlg}{\text{ lg}}
\newcommand{\tln}{\text{ ln}}
\newcommand{\tlog}{\text{ log}}

%\setlength{\parindent}{0pt}

\linespread{1.2}

\begin{document}
\arltitle{4005-800 Algorithms}{Homework 2}
\begin{prob}{1}
\end{prob}
\begin{sol} 

The time complexity of the recurrence $F_{n}$ can be characterized using a recurrence relation that defines the number of recursive calls made by $F_{n}$, as shown below.
\begin{eqnarray*}
T_{F}(0) & = & 1 \\
T_{F}(1) & = & 1 \\
T_{F}(n) & = & T_{F}(n-1) + T_{F}(n-2)
\end{eqnarray*}

The solution to $T_{F}(n)$ can be solved by treating it as a homogeneous second-order linear recurrence with constant coefficients, which yields the result that $T_{F}(n) = \frac{1}{\sqrt{5}}(\phi^{n+1} - \phi'^{n+1})$, where $\phi = \frac{1 + \sqrt{5}}{2}$ and $\phi' = \frac{1 - \sqrt{5}}{2}$. Thus, we can express this solution, and subsequently the time complexity of $F_{n}$, as a function of exponential growth of $n$, as shown below. 
\begin{eqnarray*}
T_{F}(n) \in \Theta(\phi^{n}) \implies T_{F}(n) = \Theta(\phi^{n})
\end{eqnarray*}

\end{sol}

\begin{prob}{2}
\end{prob}
\begin{sol}

The time complexity of the $fibIt$ routine can be found by solving the recurrence relation that defines $fibIt$. Such a recurrence relation can be defined by analyzing the number of additions performed during each call to $fibIt$, which is captured in the following set of equations.
\begin{eqnarray*}
T_{f}(0) & = & 0 \\
T_{f}(1) & = & 0 \\
T_{f}(n) & = & T_{f}(n-1) + 1
\end{eqnarray*}

This is because there is only one addition made in each recursive call from $f(n;a,b)$ to $f(n-1;b,a+b)$, and there are no additions made in the two cases where $n = 0$ and $n = 1$.

In order to solve this recurrence relation we can expand out the expression and attempt to identify the pattern (i.e. the \textbf{method of backwards substitution}). This process is shown below.
\begin{eqnarray*}
T_{f}(n) & = & T_{f}(n - 1) + 1 \\
& = & (T_{f}(n - 2) + 1) + 1 = T_{f}(n - 2) + 2 \\
& = & (T_{f}(n - 3) + 1) + 2 = T_{f}(n - 3) + 3 \\
& = & ... \\
& = & (T_{f}(n - k) + 1) + k = T_{f}(n - k) + k
\end{eqnarray*}

Based on this pattern, we can reach the first base case of this recurrence relation ($T_{f}(1)$) when $(n - k) = 1$, meaning that $k = (n - 1)$. Thus, we have the following.
\begin{eqnarray*}
T_{f}(n) & = & T_{f}(n - (n - 1)) + (n - 1)  \\
& = & T_{f}(1) + (n - 1) \\
& = & 0 + (n - 1) \\
& = & n - 1
\end{eqnarray*}

Based on this observation we can clearly see that $T_{f}(n) \in \Theta(n)$, or simply $T_{f}(n) = \Theta(n)$. 

%However, to verify this, we perform the substitution method on this recurrence relation. To start, we also %assume that $T_{f}(n - 1) \in \Theta(n)$. Then, we perform the induction as follows.

%\begin{eqnarray*}
%T_{f}(n) & = & T_{f}(n - 1) + 1 \\
%& = & \Theta(n) + 1 \\
%& = & n + (n - 1) \\
%& = & n - 1
%\end{eqnarray*}

%%TODO: is it necessary to perform substitution to solve the rest of this?

\end{sol}

\begin{prob}{3}
\end{prob}
\begin{sol}

\textbf{Base ($n = 0$)} \\
When $n = 0$, we know that $L^{0}(a,b) = (a,b)$ because the operator $L$ is applied $0$ times to $(a,b)$. Furthermore, by definition of $f$, we know that $(f(0;a,b), f(1;a,b)) = (a,b)$. Thus, $L^{0}(a,b) = (f(0;a,b), f(1;a,b))$. \\
%\begin{eqnarray*}
%L^{0}(a,b) & = & (f(0;a,b),f(1;a,b)) \text{ (by the induction hypothesis)}\\
%& = & (a, b) \text{ (by definition of $f$)}
%\end{eqnarray*}

\textbf{Induction ($n > 0$)} \\
First, we assume that $L^{n}(a,b) = (f(n;a,b),f(n + 1;a,b))$. Now we show that $L^{n+1}(a,b) = (f(n + 1;a,b),f(n + 2;a,b))$.
\begin{eqnarray*}
L^{n + 1}(a,b) & = & L(L^{n}(a,b)) \text{ (by law of exponents)}\\
& = & L(f(n;a,b),f(n + 1;a,b)) \text{ (by the induction hypothesis)} \\
& = & (f(n + 1;a,b), f(n;a,b) + f(n + 1;a,b)) \text{ (by definition of $L$)} \\
& = & (f(n + 1;a,b), f(n + 2;a,b)) \text{ (by Theorem 1)}
\end{eqnarray*}

Thus, $L^{n+1}(a,b) = (f(n + 1;a,b),f(n + 2;a,b))$, as desired. Therefore, we know that $f(n;a,b) = (L^{n}(a,b))_{1}$.

\end{sol}

\begin{prob}{4-a}
\end{prob}
\begin{sol}

L can be represented as the product of two matrices, as shown below.
\begin{center}
$L\begin{pmatrix} a \\ b \end{pmatrix} = \begin{pmatrix} 0 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} a \\ b \end{pmatrix} = \begin{pmatrix} b \\ a+b \end{pmatrix}$
\end{center}
\end{sol} 

\begin{prob}{4-b}
\end{prob}
\begin{sol}

We can use the method of repeated squaring to achieve fast exponentiation in $O(\tlog n)$ time. The source code for this routine (which can be implemented both recursively and iteratively) is shown below.
\begin{lstlisting}
def power(base, p):
	if (p == 0):
		return IDENTITY
	elif (p == 1):
		return base
	elif ((p % 2) == 0):
		return power(base * base, p / 2)
	else:
		return base * power(base * base, (p - 1) / 2)
\end{lstlisting}

\begin{lstlisting}
def power(base, p):
	result = IDENTITY
	while (p != 0):
		if ((p % 2) != 0):
			result = result * base
			p = p - 1
		base = base * base
		p = p / 2
	return result
\end{lstlisting}
\end{sol}

\begin{prob}{4-c}
\end{prob}
\begin{sol}

Using the representation for $L$ and the power functions described above, we can implement $fibPow$ as follows:
\begin{lstlisting}
def fibPow(n):
	base = L(0,1)
	base = power(base, n)
	return base[0]
\end{lstlisting}
\end{sol}

\begin{prob}{4-d}
\end{prob}
\begin{sol}

Since the time to perform matrix multiplication with a $2x2$ and $2x1$ matrix is constant time (i.e. $\Theta(1)$), and the multiplication routine of repeated squares that utilizes this constant operation runs in $O(\tlog n)$ time, we can conclude that the time complexity of $fibPow$ is $O(\tlog n)$.
\end{sol}

\begin{prob}{5-a}
Write down the definition of pseudo-polynomial time.
\end{prob}
\begin{sol}
\begin{define}
A pseudo-polynomial time algorithm is one that runs in polynomial time in the numeric value of its input $n$, where the value is exponential in terms of the length of $n$ (i.e. the number of bits or digits).
%\cite{BELOW}
% http://www.esi2.us.es/~mbilbao/complexi.htm
\end{define}
\end{sol}

\begin{prob}{5-b}
Is $fib$ a pseudo-polynomial time algorithm? Explain.
\end{prob}
\begin{sol}

No, $fib$ has a time complexity of $\Theta(\phi^{n})$, where $n$ is the input value, which means that it is exponential in the value of the input, which further implies that it is not polynomial in the value of the input. Therefore, by definition, $fib$ cannot be a pseudo-polynomial time algorithm.
\end{sol}

\begin{prob}{5-c}
Is $fibIt$ a pseudo-polynomial time algorithm? Explain.
\end{prob}
\begin{sol}

Yes, $fibIt$ has a time complexity of $\Theta(n)$, which can also be defined as $\Theta(2^{\tlg n})$, where $n$ is the input value, which means that it also has polynomial time complexity in the value of $n$ but exponential time in the length of $n$. Therefore, by definition, $fibIt$ must be a pseudo-polynomial time algorithm.
\end{sol}

\begin{prob}{5-d}
Is $fibPow$ a pseudo-polynomial time algorithm? Explain.
\end{prob}
\begin{sol}

No, $fibPow$ has a time complexity of $\Theta(\tlg n)$, which can also be defined as $\Theta(2^{\tlg \tlg n})$, where $n$ is the input magnitude, which means that it is sub-polynomial in the magnitude of $n$ and sub-exponential in the bit size of $n$. Therefore, by definition, $fibPow$ is not a pseudo-polynomial time algorithm.

%You can write n^(log(n)) as (k^(logk(n)))^(log(n)) = k^(K*(log(n)^2)). Since (log(n))^2 < n for n large enough, then this means that n^(log(n)) will grow slower than k^n
\end{sol}

\end{document}
