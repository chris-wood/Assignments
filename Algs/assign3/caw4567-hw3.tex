\documentclass[11pt]{article}

\usepackage{thumbpdf, amssymb, amsmath, amsthm, microtype,
	    graphicx, verbatim, listings, color, fancybox}
\usepackage[pdftex]{hyperref}
%\usepackage[margin=1in]{geometry}
\usepackage{cawsty}
\usepackage{fullpage}
\usepackage{pseudocode}
\usepackage{verbatim}

\newcommand{\tlg}{\text{ lg}}
\newcommand{\tln}{\text{ ln}}
\newcommand{\tlog}{\text{ log}}

\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}% http://ctan.org/pkg/algorithmicx
\usepackage{lipsum}% http://ctan.org/pkg/lipsum
\usepackage{xifthen}% http://ctan.org/pkg/xifthen
\usepackage{needspace}% http://ctan.org/pkg/needspace
\usepackage{hyperref}% http://ctan.org/pkg/hyperref

\usepackage{tikz}
\usetikzlibrary{arrows,%
                shapes,positioning}

\tikzstyle{vertex}=[circle,fill=black!25,minimum size=20pt,inner sep=0pt]
\tikzstyle{selected vertex} = [vertex, fill=red!24]
\tikzstyle{edge} = [draw,thick,-]
\tikzstyle{weight} = [font=\small]
\tikzstyle{selected edge} = [draw,line width=5pt,-,red!50]
\tikzstyle{ignored edge} = [draw,line width=5pt,-,black!20]

\allowdisplaybreaks[1]

% ================ ALGORITHM ENVIRONMENT ================
\newcounter{numberedAlg}% Algorithm counter
\newenvironment{numberedAlg}[1][]%
  {% \begin{numberedAlg}[#1]
    \needspace{2\baselineskip}% At least 2\baselineskip required, otherwise break
    \noindent \rule{\linewidth}{1pt} \endgraf% Top rule
    \refstepcounter{numberedAlg}% For correct reference of algorithm
    \centering \textsc{Algorithm}~\thenumberedAlg%
    \ifthenelse{\isempty{#1}}{}{:\ #1}% Typeset name (if provided)
  }{% \end{numberedAlg}
  \noindent \rule{\linewidth}{1pt}% Bottom rule
  }%

%\setlength{\parindent}{0pt}

\linespread{1.2}

\begin{document}
\cawtitle{4005-800 Algorithms}{Homework 3}
\begin{prob}{1}
CLRS 22.1-1 
\end{prob}
\begin{sol} 

Given an adjacency list representation of a \emph{directed} graph, the only way to determine the adjacent vertices of each vertex $v \in V$ is to traverse the entire adjacency list of $v$. Using this fact, we can easily determine the time complexity of computing out-degree and in-degree of every vertex as follows.

\begin{enumerate}
	\item To compute the out-degree for a single vertex $u \in V$, we must count the total amount of vertices contained within the adjacency list of $u$. To do this, we must traverse the the adjacency list for $u$, which amounts to traversing all outgoing edges starting from $u$ as well. Therefore, in order to compute the out-degree of every vertex in a directed graph, we must repeat this procedure for every vertex, which means that we will traverse over every vertex and every edge in the graph. Thus, the time complexity is to compute the out-degree of every vertex is $\Theta(V+ E)$.
	\item To compute the in-degree for a single vertex $u in V$, we must inspect all adjacency lists for every vertex $v \in V$ to determine if $u$ is adjacent to $v$. Only after a complete traversal of the entire adjacency list representation can we be certain that we have examined all possible edges leading to $u$, and thus can compute the in-degree. A naive approach to extend this to all vertices would be to repeat this search procedure $V$ times, amounting in a time complexity of $O(V(V+E))$. However, if we use an auxiliary data structure to keep track of the in-degree of every vertex $u \in V$, we need only perform this adjacency list traversal once, incrementing the in-degree of each vertex $v$ that is visited in the traversal. Therefore, just as with the out-degree calculation, the time complexity is simply $\Theta(V + E)$.
\end{enumerate}

\end{sol}

\begin{prob}{2-a}
CLRS 22.2-2
\end{prob}
\begin{sol} 

After running the breadth-first search on the undirected graph shown in Figure 22.3 in the textbook (using vertex $u$ as the source) we arrive at the following values for $d$ and $\pi$.

\begin{center}
  \begin{tabular}{| c | c  c |}
    \hline
	Vertex & $d$ & $\pi$ \\ \hline
	$r$ & 4 & $s$ \\
	$s$ & 3 & $w$ \\
	$t$ & 1 & $u$ \\
	$u$ & 0 & NIL\\ 
	$v$ & 5 & $r$\\ 
	$w$ & 2 & $t$\\
	$x$ & 1 & $u$\\
	$y$ & 1 & $u$\\ \hline
  \end{tabular}
\end{center}

\end{sol}

\begin{prob}{2-b}
CLRS 22.2-3 
\end{prob}
\begin{sol} 

The purpose of the black color is to indicate that a vertex has been completely traversed and all of its neighbors have been discovered. By removing line 18 in the BFS algorithm, we limit the vertex colors to only white and gray, which means only a single bit is necessary to store this information. We now show that removing the black color does not change the results of the BFS procedure. 

The correctness of the BFS routine is based on the usage of the queue $Q$. Vertices are only enqueued into $Q$ when they are found to be colored gray or black, but since we removed the possibility of vertices having a black color, we know that vertices are only enqueued if they are not gray. Therefore, since there is not distinction between gray and black vertices when enqueing a vertex into $Q$, the removal of the black color does not change the breadth-first behavior of the BFS routine. Furthermore, one can see that the algorithm does not explicitly depend on a vertex being colored black (i.e. there is no conditional statement in the algorithm that hinges on whether a vertex is gray or black). Therefore, by removing the color black, we are not changing the control flow of the algorithm in any way. Thus, we can conclude that removing the color black does not change $Q$ and in effect change the breadth-first traversal of a graph, nor does it change the control flow of the algorithm, so it must therefore produce the same result.

\begin{comment}
Let $v$ be a vertex in that was just added to $Q$ for the first time. If at a later point in time (after $v$ was added to $Q$) $v$ is re-added to $Q$, we now have the sequence $v, v_{1}, v_{2}, ..., v_{i}, v$ contained in $Q$. Based on the iterative approach to the BFS procedure, all of the vertices $v, v_{1}, v_{2}, ..., v_{i}$ will be colored black by the time the second instance of $v$ is dequeued from $Q$ to be processed. This has two implications:

\begin{enumerate}
	\item The number of vertices adjacent to $v$ that are enqueued upon the first visit to $v$ will be strictly larger than the number of vertices adjacent to $v$ that are enqueued upon the second visit to $v$. 
	\item The first visit to $v$ will enqueue all of the vertices in $V$ that are adjacent to $v$ following the normal breadth-first traversal approach, so any duplicate vertices that are re-added upon the second visit to $v$ will have already been added to the queue in the appropriate BFS.
\end{enumerate}
\end{comment}

%TODO: how do you say that it still proceeds in BFS manner?

\begin{comment}
Since the $Q$ still operates as before by providing first-in and first-out traversal of the vertices in $V$ (i.e. breadth-first traversal), the only difference in the BFS routine is the duplicate items in $Q$. We now show that these duplicate entries in $Q$ do not change the behavior or result of the BFS procedure.

Let $v$ be a vertex in that was just added to $Q$ for the first time. If at a later point in time (after $v$ was added to $Q$) $v$ is re-added to $Q$, where $v, v_{1}, v_{2}, ..., v_{i}, v$ is a sequence of vertices in $Q$, we know the following must be true: 

\begin{enumerate}
	\item $v, v_{1}, v_{2}, .., v_{i}$ will all be colored black by the time the second instance of $v$ is dequeued from $Q$. This is because the BFS routine will enqueue all neighbors of $v$ before continuing to the next vertex to dequeue and process in a breadth-first manner, at which point $v$ would be colored black.
	%\item All vertices $v_{1}, v_{2},...,v_{i}$ that exist in $Q$ will also be colored black by the time the second instance of $v$ is dequeued from $Q$ because the BFS routine must have dequeued all of them, enqueued their white-colored adjacent vertices, and then finished by coloring all of them black.
\end{enumerate}

Therefore, using these facts we know that upon the next visit to $v$ (i.e. after it is dequeued the next time), all of the vertices adjacent to $v$ that were contained within $\{v_{1}, v_{2},...,v_{i}\}$ will not be added to $Q$ again because they are colored black. Furthermore, using this same argument, any additional adjacant vertices $\{u : u \notin \{v_{1}, v_{2}, ..., v_{i}\}\}$ that are enqueued into $Q$ because they are not colored black must be duplicates. 

Finally, since after every traversal of the adjacency list of a vertex $v$ we are guaranteed to color the vertex black we know that any duplicate vertices that appear in $Q$ will not have any impact on the results of the BFS traversal. Furthermore, since the removal of the gray color does not change the breadth-first behavior of the traversal (because it does not modify the behavior of $Q$), we can conclude that the removal of the gray color altogether does not change the behavior of BFS; it simply poses the risk of a longer time complexity.
\end{comment}
\end{sol}

\begin{prob}{2-c}
CLRS 22.2-5 
\end{prob}
\begin{sol} 

In the correctness proof provided in the textbook, it is shown that $u.d$ = $\delta(s,u)$, meaning that $u.d$ is always equal to the length of the shortest path between $s$ and $u$ upon termination of the BFS routine. Furthermore, the proof goes on to show that the BFS routine will always produce the shortest path lengths between a start vertex $s$ and all other vertices $u \in V$ for any graph $G$ without assuming any particular order of the adjacency lists. This is intuitively true since the order of vertices in the adjacency list representation of a graph $G$ does not have any effect on the topolgy of $G$ (i.e. the actual edges that exist in the graph). Therefore, we can conclude that the value $u.d$ assigned to a vertex $u$ is independent of the order in which the vertices appear in each adjacency list for $G$.

In Figure 22.3 from the textbook, we see that $t$ must precede $u$ in the adjacency list for $w$. However, if we swap the position of $t$ and $x$ in the adjacency list for $w$, a BFS traversal will yield the edge $(x,u)$, rather than $(t,u)$, which is a different different breadth-first tree. This difference occurs because the vertices adjacent to $x$ will be enqueued in $Q$ before the vertices adjacent to $t$ because $x$ is visited first in the adjacency list. Therefore, the predecessor of $u$ will be $x$, not $t$.

\end{sol}

\begin{prob}{3}
CLRS 22.3-7
\end{prob}
\begin{sol} 

The code for the DFS algorithm that uses a stack for its depth-first traversal is shown below:

% Stack-based approach here
\begin{numberedAlg}[StackDFS]
\label{alg1}
\begin{algorithmic}[1]
	\For {each vertex $u \in G.V$}
		\State $u.color = WHITE$
		\State $u.\pi = NIL$
	\EndFor
	\State $time$ = 0
	\State $S = makeStack()$
	\For {each vertex $u \in G.V$}
		\If {$u.color == WHITE$}
			\State $S.push(u)$
			\While{$S.notEmpty()$}
				\State $time = time + 1$
				\State $v = S.top()$
				\State $S.pop()$
				\State $v.d = time$
				\State $v.color = GRAY$
				\For {each vertex $w \in G.Adj[v]$}
					\If {$w.color == WHITE$}
						\State $S.push(w)$
					\EndIf
				\EndFor
				\State $v.color = BLACK$
				\State $time = time + 1$
				\State $v.f = time$
			\EndWhile
		\EndIf
	\EndFor
\end{algorithmic}
\end{numberedAlg}
%\end{algorithm}

\begin{comment}
\begin{algorithm}                      
\caption{StackDFS}          
\label{alg1}                           
\begin{algorithmic}                   
    \Ensure $y = x^n$
    \State $y \Leftarrow 1$
    \If{$n < 0$}
        \State $X \Leftarrow 1 / x$
        \State $N \Leftarrow -n$
    \Else
        \State $X \Leftarrow x$
        \State $N \Leftarrow n$
    \EndIf
    \While{$N \neq 0$}
        \If{$N$ is even}
            \State $X \Leftarrow X \times X$
            \State $N \Leftarrow N / 2$
        \Else[$N$ is odd]
            \State $y \Leftarrow y \times X$
            \State $N \Leftarrow N - 1$
        \EndIf
    \EndWhile
\end{algorithmic}
\end{algorithm}
\end{comment}

\end{sol}

\begin{prob}{4-a}
$T(1) = 1, T(n) = aT(n-1) + bn$
\end{prob}
\begin{sol} 

To solve this recurrence relation using the iteration method, we first expand the recursive calls in order to identify a pattern, as shown below:

\begin{eqnarray*}
T(n) & = & aT(n-1) + bn \\
& = & a(aT(n-2) + b(n-1)) + bn \\
& = & a^2T(n-2) + ab(n-1) + bn\\
& = & a^2(aT(n-3) + b(n-2)) + ab(n-1) + bn\\
& = & a^3T(n-3) + a^2b(n-2) + ab(n-1) + bn \\
& = & ... \\
& = & a^kT(n-k) + a^{k-1}b(n-(k-1)) + ... + ab(n-1) + bn
\end{eqnarray*}

Now, if we let $k = (n-1)$, we will reach the end of these recursive calls and end up the following result:

\begin{eqnarray*}
T(n) & = & a^{n-1}T(n-(n-1)) + a^{n-2}b(n-(n-2)) + ... + ab(n-1) + bn \\
& = & a^{n-1} + b\sum_{i=0}^{n-2} a^i(n-i) \\
& = & a^{n-1} + bn\sum_{i=0}^{n-2} a^i - b\sum_{i=0}^{n-2} ia^i \\
& = & a^{n-1} + bn\Bigg(\frac{a^{n-1} - 1}{a-1}\Bigg) - b\Bigg(\frac{(n-2)a^{n} - (n-1)a^{n-1} + a}{(a-1)^2}\Bigg) \\
& = & a^{n-1} + \frac{bna^{n-1} - bn}{a-1} - \frac{b(n-2)a^{n} - b(n-1)a^{n-1} + ba}{(a-1)^2} 
\end{eqnarray*}

Now, by discarding all lower order constant terms and simplifying, we can see that $T(n) = O(na^n)$.

\begin{comment}
Now we make the observation that $a^n(\frac{1}{a})^{n-i} = a^{i}$, so we can re-write the summation above as $a^{n}\sum_{i=0}^{n-2} (\frac{1}{a})^{n-i}(n-i)$, which is the same as $a^{n}\Big((\sum_{i=0}^{n} i(\frac{1}{a})^i) - (\frac{1}{a})\Big)$. We now have the following:

\begin{eqnarray*}
T(n) & = & a^{n-1} + a^{n}b\Big((\sum_{i=0}^{n} i(\frac{1}{a})^i) - (\frac{1}{a})\Big)\\
& = & a^{n-1} + a^{n}b\Big(\frac{n(\frac{1}{a})^{n+2} - (n+1)(\frac{1}{a})^{n+1} + (\frac{1}{a})}{((\frac{1}{a}) - 1)^2} - (\frac{1}{a})\Big) \\
& = & a^{n-1} + \frac{a^{n}bn(\frac{1}{a})^{n+2} - a^{n}b(n+1)(\frac{1}{a})^{n+1} + a^{n}b(\frac{1}{a})}{((\frac{1}{a}) - 1)^2} - (\frac{1}{a}) \\
& = & a^{n-1} + \frac{bn(\frac{1}{a^2}) - b(n+1)(\frac{1}{a}) + a^{n - 1}b}{((\frac{1}{a}) - 1)^2} - (\frac{1}{a})
\end{eqnarray*}

By another observation we can see that the $((\frac{1}{a}) - 1)^2$ term in the demoniator of the expression above can be discarded to obtain an upper bound on T(n). Thus, we have the following:

%\begin{eqnarray*]
\begin{eqnarray*}
T(n) < a^{n-1} + bn(\frac{1}{a^2}) - b(n+1)(\frac{1}{a}) + a^{n - 1}b - (\frac{1}{a})  = (b+1)a^{n-1} + bn(\frac{1}{a^2}) - b(n+1)(\frac{1}{a}) - (\frac{1}{a}) 
\end{eqnarray*}
%\end{eqnarray*}

Therefore, by discarding the lowest terms in this expression for $T(n)$, we can conclude that $T(n) = O(a^n)$.
\end{comment}

\end{sol}

\begin{prob}{4-b}
$T(1) = 1, T(n) = aT(n-1) + bn\tlog(n)$
\end{prob}
\begin{sol} 

To solve this recurrence relation using the iteration method, we first expand the recursive calls in order to identify a pattern, as shown below:

\begin{eqnarray*}
T(n) & = & aT(n-1) + bn\tlog(n) \\
& = & a(aT(n-2) + b(n-1)\tlog(n-1)) + bn\tlog(n) \\
& = & a^2T(n-2) + ab(n-1)\tlog(n-1) + bn\tlog(n)\\
& = & a^2(aT(n-3) + b(n-2)\tlog(n-2)) + ab(n-1)\tlog(n-1) + bn\tlog(n) \\
& = & a^3T(n-3) + a^2b(n-2)\tlog(n-2) + ab(n-1)\tlog(n-1) + bn\tlog(n) \\
& = & ... \\
& = & a^kT(n-k) + a^{k-1}b(n-(k-1))\tlog(n-(k-1)) + ... + ab(n-1)\tlog(n-1) + bn\tlog(n)
\end{eqnarray*}

Now, if we let $k = (n-1)$, we will reach the end of these recursive calls and end up the following result:

\begin{eqnarray*}
T(n) & = & a^{n-1}T(n-(n-1)) + a^{n-2}b(n-(n-2))\tlog(n-(n-2)) + ... + ab(n-1)\tlog(n-1) + bn\tlog(n) \\
& = & a^{n-1}T(1) + a^{n-2}b(n-(n-2))\tlog(n-(n-2)) + ... + ab(n-1)\tlog(n-1) + bn\tlog(n) \\
& = & a^{n-1} + b\sum_{i=0}^{n-2} a^i(n-i)\tlog(n-i)
\end{eqnarray*}

Now we make the observation that $a^n(\frac{1}{a})^{n-i} = a^{i}$, so we can re-write the summation above as $a^{n}\sum_{i=0}^{n-2} \frac{1}{a}^{n-i}(n-i)\tlog(n-i)$, which is less than $a^{n}\sum_{i=0}^{n-2} (n-i)\tlog(n-i)$. Furthermore, we make the observation that $\sum_{i=0}^{n-2} \tlog(n-i) < \sum_{i=0}^{n-2} \tlog(n)$, which means we that $a^{n}\sum_{i=0}^{n-2} (n-i)\tlog(n-i) < a^{n}\tlog(n)\sum_{i=0}^{n-2} (n-i)$. We now have the following:

\begin{eqnarray*}
T(n) < a^{n-1} + a^{n}b\tlog(n)\sum_{i=0}^{n-2} (n-i) & = & a^{n-1} + a^{n}b\tlog(n)\Big(\frac{1}{2}(n-1)(n+2)\Big) \\
& = & a^{n-1} + \frac{a^{n}b\tlog(n)}{2}(n^2 + n - 2)
\end{eqnarray*}

Therefore, by discarding the lowest terms in this expression for $T(n)$, we can conclude that $T(n) = O(a^n\tlog(n)n^2)$.

\end{sol}

\begin{prob}{4-c}
$T(1) = 1, T(n) = aT(n-1) + bn^{c}$
\end{prob}
\begin{sol} 

To solve this recurrence relation using the iteration method, we first expand the recursive calls in order to identify a pattern, as shown below:

\begin{eqnarray*}
T(n) & = & aT(n-1) + bn^c \\
& = & a(aT(n-2) + b(n-1)^c) + bn^c \\
& = & a^2T(n-2) + ab(n-1)^c + bn^c\\
& = & a^2(aT(n-3) + b(n-2)^c) + ab(n-1)^c + bn^c \\
& = & a^3T(n-3) + a^2b(n-2)^c + ab(n-1)^c + bn^c \\
& = & ... \\
& = & a^kT(n-k) + a^{k-1}b(n-(k-1))^c + ... + ab(n-1)^c + bn^c
\end{eqnarray*}

Now, if we let $k = (n-1)$, we will reach the end of these recursive calls and end up the following result:

\begin{eqnarray*}
T(n) & = & a^{n-1}T(n-(n-1)) + a^{n-2}b(n-(n-2))^c + ... + ab(n-1)^c + bn^c \\
& = & a^{n-1}T(1) + a^{n-2}b(n-(n-2))^c + ... + ab(n-1)^c + bn^c \\
& = & a^{n-1} + b\sum_{i=0}^{n-2} a^i(n-i)^c
\end{eqnarray*}

Now we make the observation that $a^n(\frac{1}{a})^{n-i} = a^{i}$, so we can re-write the summation above as $a^{n}\sum_{i=0}^{n-2} \frac{1}{a}^{n-i}(n-i)^c$, which is less than $a^{n}\sum_{i=0}^{n-2} (n-i)^c$. Furthermore, we make the observation that $\sum_{i=0}^{n-2} (n-i)^c < \sum_{i=0}^{n-2} n^c$, which means we that $a^{n}\sum_{i=0}^{n-2} (n-i)^c < a^{n}\sum_{i=0}^{n-2} n^c$. We now have the following:

\begin{eqnarray*}
T(n) < a^{n-1} + a^{n}b\sum_{i=0}^{n-2} (n)^c & = & a^{n-1} + a^{n}b\Big( (n-1)n^c \Big) \\
& = & a^{n-1} + a^{n}b(n^{c+1} - n^c)
\end{eqnarray*}

Therefore, by discarding the lowest terms in this expression for $T(n)$, we can conclude that $T(n) = O(a^nn^{c+1})$.

\end{sol}

\begin{prob}{4-d}
$T(n) = aT(n/2) + bn^{c}$
\end{prob}
\begin{sol} 

To solve this recurrence relation using the iteration method, we assume that $n = 2^k$ and then continue to expand the recursive calls in order to identify a pattern, as shown below:

\begin{eqnarray*}
T(n) & = & aT(n/2) + bn^c \\
& = & a(aT(n/2^2) + b(n/2)^c) + bn^c \\
& = & a^2T(n/2^2) + ab(n/2)^c + bn^c\\
& = & a^2(aT(n/2^3) + b(n/2^2)^c) + ab(n/2)^c + bn^c \\
& = & a^3T(n/2^3) + a^2b(n/2^2)^c + ab(n/2)^c + bn^c \\
& = & ... \\
& = & a^iT(n/2^i) + a^{i-1}b(n/2^{i-1})^c + ... + ab(n/2)^c + bn^c
\end{eqnarray*}

Now, if we let $i = k$, we will reach the end of these recursive calls and end up the following result:

\begin{eqnarray*}
T(n) & = & a^{k}T(n/2^k) + a^{k-1}b(n/2^{k-1})^c + ... + ab(n/2)^c + bn^c \\
& = & a^{k}T(1) + a^{k-1}b(n/2^{k-1})^c + ... + ab(n/2)^c + bn^c \\
& = & a^{k} + b\sum_{j=0}^{k-1} a^j(n/2^j)^c
\end{eqnarray*}

By letting $n=2^k$, we can translate this result into the following:

\begin{eqnarray*}
T(2^k) & = & a^{k} + b\sum_{j=0}^{k-1} a^j2^{(k-j)c} \\
& = & a^{k} + b\sum_{j=0}^{k-1} a^j2^{kc}2^{-jc} \\
& = & a^{k} + b2^{kc}\sum_{j=0}^{k-1} a^j2^{-jc} \\
& = & a^{k} + b2^{kc}\sum_{j=0}^{k-1} (\frac{a}{2^c})^j \\
& = & a^{k} + b2^{kc}\Bigg(\frac{(\frac{a}{2^c})^k - 1}{(\frac{a}{2^c}) - 1} \Bigg) \\
& = & a^{k} + \frac{b2^{kc}(\frac{a}{2^c})^k - b2^{kc}}{(\frac{a}{2^c}) - 1} 
\end{eqnarray*}

Now, by removing the denominator from the term above and replacing $k$ with $\tlg(n)$, we have the following:

\begin{eqnarray*}
T(n) & = & a^{k} + b2^{kc}(\frac{a}{2^c})^k - b2^{kc} \\
& = & a^{\tlg(n)} + bn^c(\frac{a}{2^c})^{\tlg(n)} - bn^c \\
& = & n^{\tlg(a)} + bn^cn^{\tlg(\frac{a}{2^c})} - bn^c \\ 
& = & n^{\tlg(a)} + bn^cn^{\tlg(a) - \tlg(2^c)} - bn^c \\
& = & n^{\tlg(a)} + bn^cn^{\tlg(a) - c} - bn^c \\
& = & n^{\tlg(a)} + bn^{\tlg(a)} - bn^c \\
& = & (n+1)n^{\tlg(a)} - bn^c
\end{eqnarray*}

Now, by discarding constants, we can conclude that $T(n) = O(n^{\tlg(a)} - n^c)$, since the dominating term depends on the constants $a$ and $c$.

\begin{comment}
Now we make the observation that $a^j(\frac{n}{2^j})^{c} = a^jn^c(\frac{1}{2^c})^j = a^jn^c(\frac{1}{2^j})^c = n^c(\frac{a}{2^c})^j$, so we can re-write the summation above as $n^{c}\sum_{j=0}^{k-1} (\frac{a}{2^c})^{j}$. We now have the following:

\begin{eqnarray*}
T(n) & =&  a^{k} + n^{c}b\sum_{j=0}^{k-1} (\frac{a}{2^c})^j \\
& = & a^{k} + n^{c}b\Bigg(\frac{(\frac{a}{2^c})^k - 1}{(\frac{a}{2^c}) - 1}\Bigg) \\
& = & a^{k} + \frac{bn^c(\frac{a}{2^c})^k}{(\frac{a}{2^c}) - 1} - \frac{bn^c}{(\frac{a}{2^c}) - 1}
\end{eqnarray*}

Now, we can observe that $a^{k} + \frac{bn^c(\frac{a}{2^c})^k}{(\frac{a}{2^c}) - 1} - \frac{bn^c}{(\frac{a}{2^c}) - 1} < a^{k} + bn^c(\frac{a}{2^c})^k$. Now, by replacing $k$ with $\tlg(n)$, we have the following:

\begin{eqnarray*}
T(n) <  a^{\tlg(n)} + bn^c(\frac{a}{2^c})^{\tlg(n)} & = & n^{\tlg(a)} + bn^cn^{\tlg(\frac{a}{2^c})} \\
& = & n^{\tlg(a)} + bn^cn^{\tlg(a) - \tlg(2^c)} \\
& = & n^{\tlg(a)} + bn^cn^{\tlg(a) - c} \\
& = & n^{\tlg(a)} + bn^{\tlg(a) - c + c} \\
& = & (b+1)n^{\tlg(a)}
\end{eqnarray*}

Therefore, we can conclude that $T(n) = O(n^{\tlg(a)})$.
\end{comment}

\end{sol}

\begin{prob}{5}
\end{prob}
\begin{sol} 

The maximum element out of a set of 5 numbers can be found by iteratively applying the following equation.

\begin{eqnarray*}
\text{max}(a,b) = \frac{a+b}{2} + |\frac{a-b}{2}| = \frac{a + b + |a - b|}{2}
\end{eqnarray*}

The source code for the $max5$ routine that relies on this equation is shown below.

\begin{lstlisting}
def max5(x1, x2, x3, x4, x5):
	max1 = (x1 + x2 + abs(x1 - x2)) / 2
	max2 = (max1 + x3 + abs(max1 - x3)) / 2
	max3 = (max2 + x4 + abs(max2 - x4)) / 2
	max4 = (max3 + x5 + abs(max3 - x5)) / 2
	return max4
\end{lstlisting}

\end{sol}

%\begin{thebibliography}{}
%\bibitem{AlgoBook}
%Levitin, Anany. {\it Introduction to the Design and Analysis of Algorithms.} Pearson, Boston: 2012. Print.
%\end{thebibliography}

\end{document}
