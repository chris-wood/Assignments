\documentclass[11pt]{article}

\usepackage{thumbpdf, amssymb, amsmath, amsthm, microtype,
	    graphicx, verbatim, listings, color, fancybox}
\usepackage[pdftex]{hyperref}
%\usepackage[margin=1in]{geometry}
\usepackage{cawsty}
\usepackage{fullpage}
\usepackage{pseudocode}
\usepackage{verbatim}

\newcommand{\tlg}{\text{ lg}}
\newcommand{\tln}{\text{ ln}}
\newcommand{\tlog}{\text{ log}}

\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}% http://ctan.org/pkg/algorithmicx
\usepackage{lipsum}% http://ctan.org/pkg/lipsum
\usepackage{xifthen}% http://ctan.org/pkg/xifthen
\usepackage{needspace}% http://ctan.org/pkg/needspace
\usepackage{hyperref}% http://ctan.org/pkg/hyperref

\usepackage{tikz}
\usetikzlibrary{arrows,%
                shapes,positioning}

\tikzstyle{vertex}=[circle,fill=black!25,minimum size=20pt,inner sep=0pt]
\tikzstyle{selected vertex} = [vertex, fill=red!24]
\tikzstyle{edge} = [draw,thick,-]
\tikzstyle{weight} = [font=\small]
\tikzstyle{selected edge} = [draw,line width=5pt,-,red!50]
\tikzstyle{ignored edge} = [draw,line width=5pt,-,black!20]

\allowdisplaybreaks[1]

% ================ ALGORITHM ENVIRONMENT ================
\newcounter{numberedAlg}% Algorithm counter
\newenvironment{numberedAlg}[1][]%
  {% \begin{numberedAlg}[#1]
    \needspace{2\baselineskip}% At least 2\baselineskip required, otherwise break
    \noindent \rule{\linewidth}{1pt} \endgraf% Top rule
    \refstepcounter{numberedAlg}% For correct reference of algorithm
    \centering \textsc{Algorithm}~\thenumberedAlg%
    \ifthenelse{\isempty{#1}}{}{:\ #1}% Typeset name (if provided)
  }{% \end{numberedAlg}
  \noindent \rule{\linewidth}{1pt}% Bottom rule
  }%

%\setlength{\parindent}{0pt}

\linespread{1.2}

\begin{document}
\cawtitle{4005-800 Algorithms}{Homework 3}
\begin{prob}{1}
CLRS 22.1-1 
\end{prob}
\begin{sol} 

Given an adjacency list representation of a \emph{directed} graph, the only way to determine the adjacent vertices of each vertex $v \in V$ is to traverse the entire adjacency list of $v$. Using this fact, we can easily determine the time complexity of computing out-degree and in-degree of every vertex as follows.

\begin{enumerate}
	\item To compute the out-degree for a single vertex $u \in V$, we must count the total amount of vertices contained within the adjacency list of $u$. To do this, we must traverse the the adjacency list for $u$, which amounts to traversing all outgoing edges starting from $u$ as well. Therefore, in order to compute the out-degree of every vertex in a directed graph, we must repeat this procedure for every vertex, which means that we will traverse over every vertex and every edge in the graph. Thus, the time complexity is to compute the out-degree of every vertex is $\Theta(V+ E)$.
	\item To compute the in-degree for a single vertex $u in V$, we must inspect all adjacency lists for every vertex $v \in V$ to determine if $u$ is adjacent to $v$. Only after a complete traversal of the entire adjacency list representation can we be certain that we have examined all possible edges leading to $u$, and thus can compute the in-degree. A naive approach to extend this to all vertices would be to repeat this search procedure $V$ times, amounting in a time complexity of $O(V(V+E))$. However, if we use an auxiliary data structure to keep track of the in-degree of every vertex $u \in V$, we need only perform this adjacency list traversal once, incrementing the in-degree of each vertex $v$ that is visited in the traversal. Therefore, just as with the out-degree calculation, the time complexity is simply $\Theta(V + E)$.
\end{enumerate}

\end{sol}

\begin{prob}{2-a}
CLRS 22.2-2
\end{prob}
\begin{sol} 

After running the breadth-first search on the undirected graph shown below (INSERT IMAGE), using vertex $u$ as the source, we arrive at the following values for $d$ and $\pi$.

\begin{center}
  \begin{tabular}{| c | c  c |}
    \hline
	Vertex & $d$ & $\pi$ \\ \hline
	$r$ & 4 & $s$ \\
	$s$ & 3 & $w$ \\
	$t$ & 1 & $u$ \\
	$u$ & 0 & NIL\\ 
	$v$ & 5 & $r$\\ 
	$w$ & 2 & $t$\\
	$x$ & 1 & $u$\\
	$y$ & 1 & $y$\\ \hline
  \end{tabular}
\end{center}

\end{sol}

\begin{prob}{2-b}
CLRS 22.2-3 
\end{prob}
\begin{sol} 

The purpose of the gray colors in BFS is to indicate vertices that have been discovered in the traversal. Without this color, we are guaranteed that the only vertices added to the queue $Q$ are those that are white when visited during the traversal of a vertex adjacency list. Furthermore, the only way to avoid being added to the queue $Q$ is for the vertex under consideration to be colored black. Therefore, without the intermediate gray color, it is possible for a vertex $v$ to be added to $Q$ multiple times before its adjacency list is completely traversed and it is assigned a color black. We now show that duplicate entries in $Q$ do not change the behavior or result of the BFS procedure.

Let $v$ be a vertex in that was just added to $Q$ for the first time. If at a later point in time (after $v$ was added to $Q$) $v$ is re-added to $Q$, we now have the sequence $v, v_{1}, v_{2}, ..., v_{i}, v$ contained in $Q$. Based on the iterative approach to the BFS procedure, all of the vertices $v, v_{1}, v_{2}, ..., v_{i}$ will be colored black by the time the second instance of $v$ is dequeued from $Q$ to be processed. This has two implications:

\begin{enumerate}
	\item The number of vertices adjacent to $v$ that are enqueued upon the first visit to $v$ will be strictly larger than the number of vertices adjacent to $v$ that are enqueued upon the second visit to $v$. 
	\item The first visit to $v$ will enqueue all of the vertices in $V$ that are adjacent to $v$ following the normal breadth-first traversal approach, so any duplicate vertices that are re-added upon the second visit to $v$ will have already been added to the queue in the appropriate BFS.
\end{enumerate}

%TODO: how do you say that it still proceeds in BFS manner?

\begin{comment}
Since the $Q$ still operates as before by providing first-in and first-out traversal of the vertices in $V$ (i.e. breadth-first traversal), the only difference in the BFS routine is the duplicate items in $Q$. We now show that these duplicate entries in $Q$ do not change the behavior or result of the BFS procedure.

Let $v$ be a vertex in that was just added to $Q$ for the first time. If at a later point in time (after $v$ was added to $Q$) $v$ is re-added to $Q$, where $v, v_{1}, v_{2}, ..., v_{i}, v$ is a sequence of vertices in $Q$, we know the following must be true: 

\begin{enumerate}
	\item $v, v_{1}, v_{2}, .., v_{i}$ will all be colored black by the time the second instance of $v$ is dequeued from $Q$. This is because the BFS routine will enqueue all neighbors of $v$ before continuing to the next vertex to dequeue and process in a breadth-first manner, at which point $v$ would be colored black.
	%\item All vertices $v_{1}, v_{2},...,v_{i}$ that exist in $Q$ will also be colored black by the time the second instance of $v$ is dequeued from $Q$ because the BFS routine must have dequeued all of them, enqueued their white-colored adjacent vertices, and then finished by coloring all of them black.
\end{enumerate}

Therefore, using these facts we know that upon the next visit to $v$ (i.e. after it is dequeued the next time), all of the vertices adjacent to $v$ that were contained within $\{v_{1}, v_{2},...,v_{i}\}$ will not be added to $Q$ again because they are colored black. Furthermore, using this same argument, any additional adjacant vertices $\{u : u \notin \{v_{1}, v_{2}, ..., v_{i}\}\}$ that are enqueued into $Q$ because they are not colored black must be duplicates. 

Finally, since after every traversal of the adjacency list of a vertex $v$ we are guaranteed to color the vertex black we know that any duplicate vertices that appear in $Q$ will not have any impact on the results of the BFS traversal. Furthermore, since the removal of the gray color does not change the breadth-first behavior of the traversal (because it does not modify the behavior of $Q$), we can conclude that the removal of the gray color altogether does not change the behavior of BFS; it simply poses the risk of a longer time complexity.
\end{comment}
\end{sol}

\begin{prob}{2-c}
CLRS 22.2-5 
\end{prob}
\begin{sol} 

In the correctness proof provided in the textbook, it is shown that $u.d$ = $\delta(s,u)$, meaning that $u.d$ is always equal to the length of the shortest path between $s$ and $u$ upon termination of the BFS routine. Furthermore, the proof goes on to show that the BFS routine will always produce the shortest path lengths between a start vertex $s$ and all other vertices $u \in V$ for any graph $G$ without assuming any particular order of the adjacency lists. This is intuitively true since the order of vertices in the adjacency list representation of a graph $G$ does not have any effect on the topolgy of $G$ (i.e. the actual edges that exist in the graph). Therefore, we can conclude that the value $u.d$ assigned to a vertex $u$ is independent of the order in which the vertices appear in each adjacency list for $G$.

In Figure 22.3 from the textbook, we see that $t$ must precede $u$ in the adjacency list for $w$. However, if we swap the position of $t$ and $x$ in the adjacency list for $w$, a BFS traversal will yield the edge $(x,u)$, rather than $(t,u)$, which is a different different breadth-first tree. This difference occurs because the vertices adjacent to $x$ will be enqueued in $Q$ before the vertices adjacent to $t$ because $x$ is visited first in the adjacency list. Therefore, the predecessor of $u$ will be $x$, not $t$.

\end{sol}

\begin{prob}{3}
CLRS 22.3-7
\end{prob}
\begin{sol} 

The code for the DFS algorithm that uses a stack for its depth-first traversal is shown below:

% Stack-based approach here
\begin{numberedAlg}[StackDFS]
\label{alg1}
\begin{algorithmic}[1]
	\For {each vertex $u \in G.V$}
		\State $u.color = WHITE$
		\State $u.\pi = NIL$
	\EndFor
	\State $time$ = 0
	\State $S = makeStack()$
	\For {each vertex $u \in G.V$}
		\If {$u.color == WHITE$}
			\State $S.push(u)$
			\While{$S.notEmpty()$}
				\State $time = time + 1$
				\State $v = S.top()$
				\State $S.pop()$
				\State $v.d = time$
				\State $v.color = GRAY$
				\For {each vertex $w \in G.Adj[v]$}
					\If {$w.color == WHITE$}
						\State $S.push(w)$
					\EndIf
				\EndFor
				\State $v.color = BLACK$
				\State $time = time + 1$
				\State $v.f = time$
			\EndWhile
		\EndIf
	\EndFor
\end{algorithmic}
\end{numberedAlg}
%\end{algorithm}

\begin{comment}
\begin{algorithm}                      
\caption{StackDFS}          
\label{alg1}                           
\begin{algorithmic}                   
    \Ensure $y = x^n$
    \State $y \Leftarrow 1$
    \If{$n < 0$}
        \State $X \Leftarrow 1 / x$
        \State $N \Leftarrow -n$
    \Else
        \State $X \Leftarrow x$
        \State $N \Leftarrow n$
    \EndIf
    \While{$N \neq 0$}
        \If{$N$ is even}
            \State $X \Leftarrow X \times X$
            \State $N \Leftarrow N / 2$
        \Else[$N$ is odd]
            \State $y \Leftarrow y \times X$
            \State $N \Leftarrow N - 1$
        \EndIf
    \EndWhile
\end{algorithmic}
\end{algorithm}
\end{comment}

\end{sol}

\begin{prob}{4-a}
$T(1) = 1, T(n) = aT(n-1) + bn$
\end{prob}
\begin{sol} 

To solve this recurrence relation using the iteration method, we first expand the recursive calls in order to identify a pattern, as shown below:

\begin{eqnarray*}
T(n) & = & aT(n-1) + bn \\
& = & a(aT(n-2) + b(n-1)) + bn \\
& = & a^2T(n-2) + ab(n-1) + bn\\
& = & a^2(aT(n-3) + b(n-2)) + ab(n-1) + bn\\
& = & a^3T(n-3) + a^2b(n-2) + ab(n-1) + bn \\
& = & ... \\
& = & a^kT(n-k) + a^{k-1}b(n-(k-1)) + ... + ab(n-1) + bn
\end{eqnarray*}

Now, if we let $k = (n-1)$, we will reach the end of these recursive calls and end up the following result:

\begin{eqnarray*}
T(n) & = & a^{n-1}T(n-(n-1)) + a^{n-2}b(n-(n-2)) + ... + ab(n-1) + bn \\
& = & a^{n-1}T(1) + a^{n-2}b(n-(n-2)) + ... + ab(n-1) + bn \\
& = & a^{n-1} + b\sum_{i=0}^{n-2} a^i(n-i)
\end{eqnarray*}

Now we make the observation that $a^n(\frac{1}{a})^{n-i} = a^{i}$, so we can re-write the summation above as $a^{n}\sum_{i=0}^{n-2} (\frac{1}{a})^{n-i}(n-i)$, which is less than $a^{n}\sum_{i=0}^{n-2} (n-i)$. Furthermore, we make the observation that $\sum_{i=0}^{n-2} (n-i) = \frac{1}{2} (n-1) (n+2)$. We now have the following:

\begin{eqnarray*}
T(n) < a^{n-1} + a^{n}b\sum_{i=0}^{n-2} (n-i) & = & a^{n-1} + a^{n}b\Big(\frac{1}{2} (n-1) (n+2) \Big) \\
& = & a^{n-1} + \frac{a^{n}b}{2}(n^2 + n - 2)
\end{eqnarray*}

Therefore, by discarding the lowest terms in this expression for $T(n)$, we can conclude that $T(n) = O(a^nn^2)$.

\end{sol}

\begin{prob}{4-b}
$T(1) = 1, T(n) = aT(n-1) + bn\tlog(n)$
\end{prob}
\begin{sol} 

To solve this recurrence relation using the iteration method, we first expand the recursive calls in order to identify a pattern, as shown below:

\begin{eqnarray*}
T(n) & = & aT(n-1) + bn\tlog(n) \\
& = & a(aT(n-2) + b(n-1)\tlog(n-1)) + bn\tlog(n) \\
& = & a^2T(n-2) + ab(n-1)\tlog(n-1) + bn\tlog(n)\\
& = & a^2(aT(n-3) + b(n-2)\tlog(n-2)) + ab(n-1)\tlog(n-1) + bn\tlog(n) \\
& = & a^3T(n-3) + a^2b(n-2)\tlog(n-2) + ab(n-1)\tlog(n-1) + bn\tlog(n) \\
& = & ... \\
& = & a^kT(n-k) + a^{k-1}b(n-(k-1))\tlog(n-(k-1)) + ... + ab(n-1)\tlog(n-1) + bn\tlog(n)
\end{eqnarray*}

Now, if we let $k = (n-1)$, we will reach the end of these recursive calls and end up the following result:

\begin{eqnarray*}
T(n) & = & a^{n-1}T(n-(n-1)) + a^{n-2}b(n-(n-2))\tlog(n-(n-2)) + ... + ab(n-1)\tlog(n-1) + bn\tlog(n) \\
& = & a^{n-1}T(1) + a^{n-2}b(n-(n-2))\tlog(n-(n-2)) + ... + ab(n-1)\tlog(n-1) + bn\tlog(n) \\
& = & a^{n-1} + b\sum_{i=0}^{n-2} a^i(n-i)\tlog(n-i)
\end{eqnarray*}

Now we make the observation that $a^n(\frac{1}{a})^{n-i} = a^{i}$, so we can re-write the summation above as $a^{n}\sum_{i=0}^{n-2} \frac{1}{a}^{n-i}(n-i)\tlog(n-i)$, which is less than $a^{n}\sum_{i=0}^{n-2} (n-i)\tlog(n-i)$. Furthermore, we make the observation that $\sum_{i=0}^{n-2} \tlog(n-i) < \sum_{i=0}^{n-2} \tlog(n)$, which means we that $a^{n}\sum_{i=0}^{n-2} (n-i)\tlog(n-i) < a^{n}\tlog(n)\sum_{i=0}^{n-2} (n-i)$. We now have the following:

\begin{eqnarray*}
T(n) < a^{n-1} + a^{n}b\tlog(n)\sum_{i=0}^{n-2} (n-i) & = & a^{n-1} + a^{n}b\tlog(n)\Big(\frac{1}{2}(n-1)(n+2)\Big) \\
& = & a^{n-1} + \frac{a^{n}b\tlog(n)}{2}(n^2 + n - 2)
\end{eqnarray*}

Therefore, by discarding the lowest terms in this expression for $T(n)$, we can conclude that $T(n) = O(a^n\tlog(n)n^2)$.

\end{sol}

\begin{prob}{4-c}
$T(1) = 1, T(n) = aT(n-1) + bn^{c}$
\end{prob}
\begin{sol} 

To solve this recurrence relation using the iteration method, we first expand the recursive calls in order to identify a pattern, as shown below:

\begin{eqnarray*}
T(n) & = & aT(n-1) + bn^c \\
& = & a(aT(n-2) + b(n-1)^c) + bn^c \\
& = & a^2T(n-2) + ab(n-1)^c + bn^c\\
& = & a^2(aT(n-3) + b(n-2)^c) + ab(n-1)^c + bn^c \\
& = & a^3T(n-3) + a^2b(n-2)^c + ab(n-1)^c + bn^c \\
& = & ... \\
& = & a^kT(n-k) + a^{k-1}b(n-(k-1))^c + ... + ab(n-1)^c + bn^c
\end{eqnarray*}

Now, if we let $k = (n-1)$, we will reach the end of these recursive calls and end up the following result:

\begin{eqnarray*}
T(n) & = & a^{n-1}T(n-(n-1)) + a^{n-2}b(n-(n-2))^c + ... + ab(n-1)^c + bn^c \\
& = & a^{n-1}T(1) + a^{n-2}b(n-(n-2))^c + ... + ab(n-1)^c + bn^c \\
& = & a^{n-1} + b\sum_{i=0}^{n-2} a^i(n-i)^c
\end{eqnarray*}

Now we make the observation that $a^n(\frac{1}{a})^{n-i} = a^{i}$, so we can re-write the summation above as $a^{n}\sum_{i=0}^{n-2} \frac{1}{a}^{n-i}(n-i)^c$, which is less than $a^{n}\sum_{i=0}^{n-2} (n-i)^c$. Furthermore, we make the observation that $\sum_{i=0}^{n-2} (n-i)^c < \sum_{i=0}^{n-2} n^c$, which means we that $a^{n}\sum_{i=0}^{n-2} (n-i)^c < a^{n}\sum_{i=0}^{n-2} n^c$. We now have the following:

\begin{eqnarray*}
T(n) < a^{n-1} + a^{n}b\sum_{i=0}^{n-2} (n)^c & = & a^{n-1} + a^{n}b\Big( (n-1)n^c \Big) \\
& = & a^{n-1} + a^{n}b(n^{c+1} - n^c)
\end{eqnarray*}

Therefore, by discarding the lowest terms in this expression for $T(n)$, we can conclude that $T(n) = O(a^nn^{c+1})$.

\end{sol}

\begin{prob}{4-d}
$T(n) = aT(n/2) + bn^{c}$
\end{prob}
\begin{sol} 

To solve this recurrence relation using the iteration method, we assume that $n = 2^k$ and then continue to expand the recursive calls in order to identify a pattern, as shown below:

\begin{eqnarray*}
T(n) & = & aT(n/2) + bn^c \\
& = & a(aT(n/2^2) + b(n/2)^c) + bn^c \\
& = & a^2T(n/2^2) + ab(n/2)^c + bn^c\\
& = & a^2(aT(n/2^3) + b(n/2^2)^c) + ab(n/2)^c + bn^c \\
& = & a^3T(n/2^3) + a^2b(n/2^2)^c + ab(n/2)^c + bn^c \\
& = & ... \\
& = & a^iT(n/2^i) + a^{i-1}b(n/2^{i-1})^c + ... + ab(n/2)^c + bn^c
\end{eqnarray*}

Now, if we let $i = k$, we will reach the end of these recursive calls and end up the following result:

\begin{eqnarray*}
T(n) & = & a^{k}T(n/2^k) + a^{k-1}b(n/2^{k-1})^c + ... + ab(n/2)^c + bn^c \\
& = & a^{k}T(1) + a^{k-1}b(n/2^{k-1})^c + ... + ab(n/2)^c + bn^c \\
& = & a^{k} + b\sum_{j=0}^{k-1} a^j(n/2^j)^c
\end{eqnarray*}

Now we make the observation that $a^j(\frac{n}{2^j})^{c} = a^jn^c(\frac{1}{2^c})^j = a^jn^c(\frac{1}{2^j})^c = n^c(\frac{a}{2^c})^j$, so we can re-write the summation above as $n^{c}\sum_{j=0}^{k-1} (\frac{a}{2^c})^{j}$. We now have the following:

\begin{eqnarray*}
T(n) & =&  a^{k} + n^{c}b\sum_{j=0}^{k-1} (\frac{a}{2^c})^j \\
& = & a^{k} + n^{c}b\Bigg(\frac{(\frac{a}{2^c})^k - 1}{(\frac{a}{2^c}) - 1}\Bigg) \\
& = & a^{k} + \frac{bn^c(\frac{a}{2^c})^k}{(\frac{a}{2^c}) - 1} - \frac{bn^c}{(\frac{a}{2^c}) - 1}
\end{eqnarray*}

Now, we can observe that $a^{k} + \frac{bn^c(\frac{a}{2^c})^k}{(\frac{a}{2^c}) - 1} - \frac{bn^c}{(\frac{a}{2^c}) - 1} < a^{k} + bn^c(\frac{a}{2^c})^k$. Now, by replacing $k$ with $\tlg(n)$, we have the following:

\begin{eqnarray*}
T(n) <  a^{\tlg(n)} + bn^c(\frac{a}{2^c})^{\tlg(n)} & = & n^{\tlg(a)} + bn^cn^{\tlg(\frac{a}{2^c})} \\
& = & n^{\tlg(a)} + bn^cn^{\tlg(a) - \tlg(2^c)} \\
& = & n^{\tlg(a)} + bn^cn^{\tlg(a) - c} \\
& = & n^{\tlg(a)} + bn^{\tlg(a) - c + c} \\
& = & (b+1)n^{\tlg(a)}
\end{eqnarray*}

Therefore, we can conclude that $T(n) = O(n^{\tlg(a)})$.

\end{sol}

\begin{prob}{5}
\end{prob}
\begin{sol} 

The maximum element out of a set of 5 numbers can be found by iteratively applying the following equation.

\begin{eqnarray*}
\text{max}(a,b) = \frac{a+b}{2} + |\frac{a-b}{2}| = \frac{a + b + |a - b|}{2}
\end{eqnarray*}

The source code for the $max5$ routine that relies on this equation is shown below.

\begin{lstlisting}
def max5(x1, x2, x3, x4, x5):
	max1 = (x1 + x2 + abs(x1 - x2)) / 2
	max2 = (max1 + x3 + abs(max1 - x3)) / 2
	max3 = (max2 + x4 + abs(max2 - x4)) / 2
	max4 = (max3 + x5 + abs(max3 - x5)) / 2
	return max4
\end{lstlisting}

\end{sol}

%\begin{thebibliography}{}
%\bibitem{AlgoBook}
%Levitin, Anany. {\it Introduction to the Design and Analysis of Algorithms.} Pearson, Boston: 2012. Print.
%\end{thebibliography}

\end{document}
